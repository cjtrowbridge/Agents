{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Agent Notebook\nThis notebook demonstrates an agent workflow using autogen and models hosted on Ollama."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "!pip install -qU     autogen     autogen-ext[ollama]     jupytext markdownify     accelerate bitsandbytes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Imports and constants"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "from autogen import AssistantAgent, UserProxyAgent, GroupChat\nfrom autogen import OpenAIChatCompletionClient\nimport pathlib, datetime, uuid, subprocess, shlex, markdownify\n\nREPO_ROOT = pathlib.Path('/srv/notebooks')\nLOG_DIR = REPO_ROOT / 'agent_logs'\nLOG_DIR.mkdir(exist_ok=True, parents=True)\nBASE_URL = 'http://docker-ai:11434/v1'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model clients"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "qwen14_client = OpenAIChatCompletionClient(\n    model='qwen2.5-14b-1m',\n    base_url=BASE_URL,\n    api_key='ollama')\n\nqwen32_client = OpenAIChatCompletionClient(\n    model='qwen3-32b-5k_s',\n    base_url=BASE_URL,\n    api_key='ollama')\n\ndevstral_client = OpenAIChatCompletionClient(\n    model='devstral-24b-5k_s',\n    base_url=BASE_URL,\n    api_key='ollama')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Markdown logger"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "def log_markdown(task_id, role, content):\n    ts = datetime.datetime.utcnow().isoformat()\n    fn = LOG_DIR / f'{task_id}.md'\n    if not fn.exists():\n        with open(fn, 'w') as f:\n            f.write(f'---\nid: {task_id}\ncreated: {ts}\n---\n\n')\n    with open(fn, 'a') as f:\n        f.write(f'### {ts} \u2014 {role}\n\n' + markdownify.markdownify(content) + '\n\n')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Shell helper"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "def run_shell(cmd: str) -> str:\n    out = subprocess.check_output(shlex.split(cmd), text=True, timeout=900, stderr=subprocess.STDOUT)\n    return f'```shell\n$ {cmd}\n{out}\n```'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Agent declarations"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "planner = AssistantAgent(\n    name='planner',\n    llm_config={'client': qwen14_client, 'temperature': 0.3},\n    system_message=(\"You are a project planner. Break the user's request into a YAML list of\n        atomic tasks. Stop when each sub-task can be executed in one short\n        Python call or shell command inside the current Jupyter kernel.\"))\n\nworker = AssistantAgent(\n    name='worker',\n    llm_config={'client': qwen32_client, 'temperature': 0},\n    system_message='Execute the given atomic task and return result.')\nworker.register_function(run_shell)\n\ncoder = AssistantAgent(\n    name='coder',\n    llm_config={'client': devstral_client, 'temperature': 0},\n    system_message='You are a senior software engineer. Write, refactor, and debug code snippets as requested.')\n\nreviewer = AssistantAgent(\n    name='reviewer',\n    llm_config={'client': qwen14_client, 'temperature': 0},\n    system_message=(\"Evaluate the worker or coder output against the task description.\n        If incorrect, respond with REVISE and instructions; otherwise APPROVED.\"))\n\nagents = [planner, worker, coder, reviewer]\ngroup = GroupChat(agents=agents, max_round=30)\nproxy = UserProxyAgent(groupchat=group, human_input_mode='NEVER')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Driver function"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "def run_agent(prompt: str):\n    task_id = uuid.uuid4().hex[:8]\n    log_markdown(task_id, 'USER', prompt)\n    proxy.initiate_chat(prompt=prompt)\n    for m in group.chat_history:\n        log_markdown(task_id, m['role'], m['content'])\n    return LOG_DIR / f'{task_id}.md'"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Example call"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "run_agent('Generate Python code to scrape example.com daily and store results in SQLite \u2026')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Git auto-commit"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "!git add agent_logs/*.md && (git diff --cached --quiet || git commit -m 'agent run') && git push"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Version info"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "source": "!pip freeze | grep -E '(autogen|transformers|ollama)'"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

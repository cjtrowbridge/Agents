{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Agent Notebook\n",
        "This notebook demonstrates an agent workflow using autogen and models hosted on Ollama."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U     autogen     autogen-ext[ollama]     jupytext markdownify     accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and constants"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from autogen import AssistantAgent, UserProxyAgent, GroupChat\n",
        "# local fallback client if library missing or openai unavailable\n",
        "try:\n",
        "    from autogen import OpenAIChatCompletionClient as _OAIClient\n",
        "    import openai\n",
        "    OpenAIChatCompletionClient = _OAIClient\n",
        "except Exception:\n",
        "    class OpenAIChatCompletionClient:\n",
        "        def __init__(self, model, base_url=None, api_key=None):\n",
        "            self.model = model\n",
        "            self.base_url = base_url\n",
        "            self.api_key = api_key\n",
        "        def chat(self, messages, temperature=0):\n",
        "            raise NotImplementedError('OpenAIChatCompletionClient is unavailable')\n",
        "import pathlib, datetime, uuid, subprocess, shlex, markdownify\n",
        "\n",
        "REPO_ROOT = pathlib.Path.cwd()\n",
        "LOG_DIR = REPO_ROOT / 'agent_logs'\n",
        "LOG_DIR.mkdir(exist_ok=True, parents=True)\n",
        "BASE_URL = 'http://docker-ai:11434/v1'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model clients"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qwen14_client = OpenAIChatCompletionClient(\n",
        "    model='qwen2.5-14b-1m',\n",
        "    base_url=BASE_URL,\n",
        "    api_key='ollama')\n",
        "\n",
        "qwen32_client = OpenAIChatCompletionClient(\n",
        "    model='qwen3-32b-5k_s',\n",
        "    base_url=BASE_URL,\n",
        "    api_key='ollama')\n",
        "\n",
        "devstral_client = OpenAIChatCompletionClient(\n",
        "    model='devstral-24b-5k_s',\n",
        "    base_url=BASE_URL,\n",
        "    api_key='ollama')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Markdown logger"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def log_markdown(task_id, role, content):\n",
        "    ts = datetime.datetime.utcnow().isoformat()\n",
        "    fn = LOG_DIR / f\"{task_id}.md\"\n",
        "    if not fn.exists():\n",
        "        with open(fn, 'w') as f:\n",
        "            f.write(f'---\\nid: {task_id}\\ncreated: {ts}\\n---\\n\\n')\n",
        "    with open(fn, 'a') as f:\n",
        "        f.write(f'### {ts} — {role}\\n\\n{markdownify.markdownify(content)}\\n\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shell helper"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_shell(cmd: str) -> str:\n",
        "    out = subprocess.check_output(shlex.split(cmd), text=True, timeout=900, stderr=subprocess.STDOUT)\n",
        "    return f'```shell\\n$ {cmd}\\n{out}\\n```'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent declarations"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "planner = AssistantAgent(\n",
        "    name='planner',\n",
        "    llm_config={'config_list': [{ 'model': qwen14_client.model, 'base_url': qwen14_client.base_url, 'api_key': qwen14_client.api_key }], 'temperature': 0.3},\n",
        "    system_message=(\"You are a project planner. Break the user's request into a YAML list of atomic tasks. Stop when each sub-task can be executed in one short Python call or shell command inside the current Jupyter kernel.\"),\n",
        ")\n",
        "\n",
        "worker = AssistantAgent(\n",
        "    name='worker',\n",
        "    llm_config={'config_list': [{ 'model': qwen32_client.model, 'base_url': qwen32_client.base_url, 'api_key': qwen32_client.api_key }], 'temperature': 0},\n",
        "    system_message='Execute the given atomic task and return result.')\n",
        "worker.register_function(run_shell)\n",
        "\n",
        "coder = AssistantAgent(\n",
        "    name='coder',\n",
        "    llm_config={'config_list': [{ 'model': devstral_client.model, 'base_url': devstral_client.base_url, 'api_key': devstral_client.api_key }], 'temperature': 0},\n",
        "    system_message='You are a senior software engineer. Write, refactor, and debug code snippets as requested.')\n",
        "\n",
        "reviewer = AssistantAgent(\n",
        "    name='reviewer',\n",
        "    llm_config={'config_list': [{ 'model': qwen14_client.model, 'base_url': qwen14_client.base_url, 'api_key': qwen14_client.api_key }], 'temperature': 0},\n",
        "    system_message=(\"Evaluate the worker or coder output against the task description. If incorrect, respond with REVISE and instructions; otherwise APPROVED.\"),\n",
        ")\n",
        "\n",
        "agents = [planner, worker, coder, reviewer]\n",
        "group = GroupChat(agents=agents, max_round=30)\n",
        "proxy = UserProxyAgent(groupchat=group, human_input_mode='NEVER')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Driver function"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_agent(prompt: str):\n",
        "    task_id = uuid.uuid4().hex[:8]\n",
        "    log_markdown(task_id, 'USER', prompt)\n",
        "    proxy.initiate_chat(prompt=prompt)\n",
        "    for m in group.chat_history:\n",
        "        log_markdown(task_id, m['role'], m['content'])\n",
        "    return LOG_DIR / f'{task_id}.md'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example call"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_agent('Generate Python code to scrape example.com daily and store results in SQLite …')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Git auto-commit"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git add agent_logs/*.md && (git diff --cached --quiet || git commit -m 'agent run') && git push"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Version info"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip freeze | grep -E '(autogen|transformers|ollama)'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

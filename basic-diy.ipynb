{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Notebook\n",
    "This notebook demonstrates an agent workflow using autogen and models hosted on Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util, subprocess, sys\n",
    "\n",
    "packages = {\n",
    "    'autogen': 'autogen',\n",
    "    'ag2[openai]': 'ag2',\n",
    "    'autogen-ext[ollama]': 'autogen.ext',\n",
    "    'jupytext': 'jupytext',\n",
    "    'markdownify': 'markdownify',\n",
    "    'accelerate': 'accelerate',\n",
    "    'bitsandbytes': 'bitsandbytes',\n",
    "    'requests': 'requests',\n",
    "}\n",
    "\n",
    "for pkg, module in packages.items():\n",
    "    if importlib.util.find_spec(module) is None:\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-U', pkg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager\n",
    "\n",
    "#from autogen import OpenAIChatCompletionClient as _OAIClient\n",
    "import openai\n",
    "#OpenAIChatCompletionClient = _OAIClient\n",
    "\n",
    "import os, pathlib, datetime, uuid, subprocess, shlex, markdownify, sys, io, contextlib\n",
    "\n",
    "REPO_ROOT = pathlib.Path.cwd()\n",
    "LOG_DIR = REPO_ROOT / 'agent_logs'\n",
    "LOG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "BASE_URL = os.environ.get('OLLAMA_BASE_URL', 'http://docker-ai:11434/v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Ollama Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama models: Qwen2.5-14B-Instruct-1M-Q8_0:latest, gguf/DeepSeek-Janus-Pro-7B:Q8_0, llama3.2-vision:11b, Qwen3-32B-Q5_0:latest, deepseek-ai_DeepSeek-R1-0528-Qwen3-8B-Q8_0:latest, Llama-3.2-3B-Instruct-Q8_0:latest, qwq:32b, qwen3:32b, devstral:24b, deepseek-r1:8b, gemma3:27b\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def ensure_ollama():\n",
    "    url=f'{BASE_URL}/models'\n",
    "    try:\n",
    "        resp=requests.get(url, timeout=5)\n",
    "        resp.raise_for_status()\n",
    "        data=resp.json()\n",
    "        names=[m.get('id') or m.get('name') for m in data.get('data', data.get('models', []))]\n",
    "        print('Ollama models:', ', '.join(names))\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f'Ollama server not reachable at {BASE_URL}: {e}')\n",
    "\n",
    "ensure_ollama()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen14_client = OpenAIChatCompletionClient(\n",
    "    model='Qwen2.5-14B-Instruct-1M-Q8_0:latest',\n",
    "    base_url=BASE_URL,\n",
    "    api_key='ollama')\n",
    "\n",
    "qwen32_client = OpenAIChatCompletionClient(\n",
    "    model='Qwen3-32B-Q5_0:latest',\n",
    "    base_url=BASE_URL,\n",
    "    api_key='ollama')\n",
    "\n",
    "devstral_client = OpenAIChatCompletionClient(\n",
    "    model='devstral:24b',\n",
    "    base_url=BASE_URL,\n",
    "    api_key='ollama')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdown logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_markdown(log_dir: pathlib.Path, role, content, print_stdout=True):\n",
    "    ts = datetime.datetime.utcnow().isoformat()\n",
    "    fn = log_dir / 'log.md'\n",
    "    if not fn.exists():\n",
    "        log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        with open(fn, 'w') as f:\n",
    "            f.write(f'---\\nid: {log_dir.name}\\ncreated: {ts}\\n---\\n\\n')\n",
    "    with open(fn, 'a') as f:\n",
    "        f.write(f'### {ts} — {role}\\n\\n{markdownify.markdownify(content)}\\n\\n')\n",
    "    if print_stdout:\n",
    "        print(f'[{ts}] {role}: {content}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shell helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_shell(cmd: str) -> str:\n",
    "    out = subprocess.check_output(shlex.split(cmd), text=True, timeout=900, stderr=subprocess.STDOUT)\n",
    "    return f'```shell\\n$ {cmd}\\n{out}\\n```'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent declarations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = AssistantAgent(\n",
    "    name='planner',\n",
    "    llm_config={'config_list': [{ 'model': qwen14_client.model, 'base_url': qwen14_client.base_url, 'api_key': qwen14_client.api_key }], 'temperature': 0.3},\n",
    "    system_message=(\"You are a project planner. Break the user's request into a YAML list of atomic tasks. Stop when each sub-task can be executed in one short Python call or shell command inside the current Jupyter kernel.\"),\n",
    ")\n",
    "\n",
    "worker = AssistantAgent(\n",
    "    name='worker',\n",
    "    llm_config={'config_list': [{ 'model': qwen32_client.model, 'base_url': qwen32_client.base_url, 'api_key': qwen32_client.api_key }], 'temperature': 0},\n",
    "    system_message='Execute the given atomic task and return result.')\n",
    "worker.register_function({'run_shell': run_shell})\n",
    "\n",
    "coder = AssistantAgent(\n",
    "    name='coder',\n",
    "    llm_config={'config_list': [{ 'model': devstral_client.model, 'base_url': devstral_client.base_url, 'api_key': devstral_client.api_key }], 'temperature': 0},\n",
    "    system_message='You are a senior software engineer. Write, refactor, and debug code snippets as requested.')\n",
    "\n",
    "reviewer = AssistantAgent(\n",
    "    name='reviewer',\n",
    "    llm_config={'config_list': [{ 'model': qwen14_client.model, 'base_url': qwen14_client.base_url, 'api_key': qwen14_client.api_key }], 'temperature': 0},\n",
    "    system_message=(\"Evaluate the worker or coder output against the task description. If incorrect, respond with REVISE and instructions; otherwise APPROVED.\"),\n",
    ")\n",
    "\n",
    "agents = [planner, worker, coder, reviewer]\n",
    "group = GroupChat(agents=agents, max_round=30)\n",
    "manager = GroupChatManager(groupchat=group, llm_config={'config_list': [{ 'model': qwen14_client.model, 'base_url': qwen14_client.base_url, 'api_key': qwen14_client.api_key }], 'temperature': 0})\n",
    "proxy = UserProxyAgent(name='user', human_input_mode='NEVER', code_execution_config={'use_docker': False})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(prompt: str):\n",
    "    run_dir = LOG_DIR / datetime.datetime.utcnow().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "    log_markdown(run_dir, 'INFO', 'Agent run started')\n",
    "    log_markdown(run_dir, 'USER', prompt)\n",
    "    class LogStream(io.TextIOBase):\n",
    "        def __init__(self):\n",
    "            self._stdout = sys.__stdout__\n",
    "        def write(self, s):\n",
    "            if s:\n",
    "                self._stdout.write(s)\n",
    "                if s.strip():\n",
    "                    log_markdown(run_dir, 'VERBOSE', s, print_stdout=False)\n",
    "            return len(s)\n",
    "        def flush(self):\n",
    "            self._stdout.flush()\n",
    "    stream = LogStream()\n",
    "    with contextlib.redirect_stdout(stream), contextlib.redirect_stderr(stream):\n",
    "        try:\n",
    "            proxy.initiate_chat(manager, message=prompt)\n",
    "        except Exception as e:\n",
    "            log_markdown(run_dir, 'ERROR', f'LLM request failed: {e}')\n",
    "            raise\n",
    "    for m in group.chat_history:\n",
    "        log_markdown(run_dir, m['role'], m['content'])\n",
    "    log_markdown(run_dir, 'INFO', 'Agent run complete')\n",
    "    return run_dir / 'log.md'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CJ\\AppData\\Local\\Temp\\ipykernel_3856\\400404312.py:2: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  run_dir = LOG_DIR / datetime.datetime.utcnow().strftime('%Y-%m-%d-%H-%M-%S-%f')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-07T05:51:58.268411] INFO: Agent run started\n",
      "[2025-07-07T05:51:58.269322] USER: Generate Python code to scrape example.com daily and store results in SQLite …\n"
     ]
    }
   ],
   "source": [
    "#run_agent('Generate Python code to scrape example.com daily and store results in SQLite …')\n",
    "run_agent('Analyze all the files in the papers directory recursively. For each paper, analyze this notebook and make recommendations based on the arguments made in the paper Save each in a separate file within the agent_logs directory for this run.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git auto-commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add agent_logs/** && (git diff --cached --quiet || git commit -m 'agent run') && git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep -E '(autogen|transformers|ollama)'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
